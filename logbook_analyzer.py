import streamlit as st
import pandas as pd
import numpy as np
import faiss
import requests
import json
import re
from datetime import datetime
import logging
from typing import List, Tuple, Optional
import pickle
import os
import sys
import io
from contextlib import redirect_stdout, redirect_stderr
import time

# Torch Import Probleme vermeiden
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# Custom Logger fÃ¼r Streamlit
class StreamlitLogHandler(logging.Handler):
    def __init__(self):
        super().__init__()
        self.logs = []
    
    def emit(self, record):
        log_entry = self.format(record)
        self.logs.append(f"{datetime.now().strftime('%H:%M:%S')} - {log_entry}")
        # Nur die letzten 50 Logs behalten
        if len(self.logs) > 50:
            self.logs = self.logs[-50:]

# Sentence Transformers Import nach anderen Imports
try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    st.error("sentence-transformers nicht installiert. Bitte installieren Sie es mit: pip install sentence-transformers")
    st.stop()

# Logging Setup mit Streamlit Integration
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Streamlit Log Handler
if 'log_handler' not in st.session_state:
    st.session_state.log_handler = StreamlitLogHandler()
    st.session_state.log_handler.setFormatter(
        logging.Formatter('%(levelname)s - %(message)s')
    )
    logger.addHandler(st.session_state.log_handler)

class LogbookAnalyzer:
    def __init__(self):
        self.model = None
        self.index = None
        self.df = None
        self.embeddings = None
        self.model_name = 'intfloat/multilingual-e5-small'
        self.ollama_status = "unbekannt"
        self.ollama_model = "llama3.1:8b"
        
    def auto_initialize(self):
        """Automatische Initialisierung beim App-Start"""
        try:
            # Embedding-Modell laden
            with st.spinner('ðŸ¤– Lade Embedding-Modell...'):
                success = self.load_embedding_model()
                if success:
                    st.success("âœ… Embedding-Modell geladen")
                    logger.info("Embedding-Modell erfolgreich geladen")
                else:
                    st.error("âŒ Embedding-Modell konnte nicht geladen werden")
                    return False
            
            # Ollama Status prÃ¼fen
            with st.spinner('ðŸ¦™ PrÃ¼fe Ollama-Verbindung...'):
                self.check_ollama_status()
                
            return True
            
        except Exception as e:
            logger.error(f"Fehler bei Auto-Initialisierung: {e}")
            st.error(f"Fehler bei der Initialisierung: {e}")
            return False
        
    def load_embedding_model(self):
        """LÃ¤dt das Embedding-Modell"""
        try:
            self.model = SentenceTransformer(self.model_name)
            logger.info(f"Embedding-Modell {self.model_name} geladen")
            return True
        except Exception as e:
            logger.error(f"Fehler beim Laden des Embedding-Modells: {e}")
            return False
    
    def check_ollama_status(self):
        """PrÃ¼ft Ollama-Status und verfÃ¼gbare Modelle"""
        try:
            # Teste verschiedene Endpunkte
            test_urls = [
                "http://localhost:11434/api/tags",
                "http://127.0.0.1:11434/api/tags"
            ]
            
            for url in test_urls:
                try:
                    response = requests.get(url, timeout=5)
                    if response.status_code == 200:
                        models_data = response.json()
                        available_models = [model['name'] for model in models_data.get('models', [])]
                        
                        if self.ollama_model in available_models:
                            # Test-Query senden
                            test_success = self.test_ollama_model()
                            if test_success:
                                self.ollama_status = "âœ… Aktiv und funktionsfÃ¤hig"
                                st.success(f"ðŸ¦™ Ollama: {self.ollama_status}")
                                logger.info(f"Ollama funktioniert mit Modell {self.ollama_model}")
                            else:
                                self.ollama_status = "âš ï¸ Verbunden, aber Modell antwortet nicht"
                                st.warning(f"ðŸ¦™ Ollama: {self.ollama_status}")
                        else:
                            self.ollama_status = f"âš ï¸ Modell '{self.ollama_model}' nicht verfÃ¼gbar"
                            st.warning(f"ðŸ¦™ Ollama: {self.ollama_status}")
                            if available_models:
                                st.info(f"VerfÃ¼gbare Modelle: {', '.join(available_models[:3])}")
                        return
                        
                except requests.exceptions.RequestException:
                    continue
            
            # Alle URLs fehlgeschlagen
            self.ollama_status = "âŒ Server nicht erreichbar"
            st.error(f"ðŸ¦™ Ollama: {self.ollama_status}")
            st.info("ðŸ’¡ Starten Sie Ollama mit: `ollama serve`")
            
        except Exception as e:
            self.ollama_status = f"âŒ Fehler: {str(e)[:50]}"
            st.error(f"ðŸ¦™ Ollama: {self.ollama_status}")
            logger.error(f"Ollama-Status-Check fehlgeschlagen: {e}")
    
    def test_ollama_model(self) -> bool:
        """Testet das Ollama-Modell mit einer einfachen Anfrage"""
        try:
            test_prompt = "Antworte nur mit 'OK'"
            response = self.query_ollama(test_prompt, "", timeout=10)
            
            # PrÃ¼fe ob Response sinnvoll ist
            if response and "OK" in response and "âŒ" not in response:
                return True
            return False
            
        except Exception as e:
            logger.error(f"Ollama-Modell-Test fehlgeschlagen: {e}")
            return False
    
    def preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Bereinigt und validiert die CSV-Daten"""
        try:
            # Spalten validieren und normalisieren - Ihre Logbuch-Struktur
            expected_columns = ['Datum', 'Zeit', 'Lot-Nr.', 'Subsystem', 'Ereignis & Massnahme', 'Visum']
            
            # Flexible Spaltenerkennung
            df_columns = df.columns.tolist()
            column_mapping = {}
            
            # Mapping-Dictionary fÃ¼r verschiedene Varianten
            column_variants = {
                'Datum': ['datum', 'date', 'time-stamp', 'zeitstempel'],
                'Zeit': ['zeit', 'time', 'uhrzeit'],
                'Lot-Nr.': ['lot', 'lotnummer', 'lot-nr', 'lot-nummer', 'charge'],
                'Subsystem': ['subsystem', 'system', 'bereich', 'modul'],
                'Ereignis & Massnahme': ['ereignis', 'eintrag', 'massnahme', 'entry', 'beschreibung', 'inhalt'],
                'Visum': ['visum', 'signature', 'unterschrift', 'benutzer', 'user']
            }
            
            for expected in expected_columns:
                # Exakte Ãœbereinstimmung
                if expected in df_columns:
                    column_mapping[expected] = expected
                else:
                    # Ã„hnliche Spalten finden
                    found = False
                    for col in df_columns:
                        col_lower = col.lower().strip()
                        # PrÃ¼fe Varianten fÃ¼r diese Spalte
                        variants = column_variants.get(expected, [expected.lower()])
                        if any(variant in col_lower for variant in variants):
                            column_mapping[expected] = col
                            found = True
                            break
                    
                    if not found:
                        # Fallback: beste Ãœbereinstimmung finden
                        for col in df_columns:
                            if expected.lower().replace('-', '').replace(' ', '') in col.lower().replace('-', '').replace(' ', ''):
                                column_mapping[expected] = col
                                break
            
            # Fehlende Spalten prÃ¼fen
            missing_cols = [col for col in expected_columns if col not in column_mapping]
            if missing_cols:
                st.warning(f"Fehlende Spalten werden mit Platzhaltern gefÃ¼llt: {missing_cols}")
                for col in missing_cols:
                    df[col] = 'N/A'
                    column_mapping[col] = col
            
            # DataFrame mit korrekten Spaltennamen erstellen
            processed_df = pd.DataFrame()
            for expected, actual in column_mapping.items():
                processed_df[expected] = df[actual]
            
            # Datum und Zeit kombinieren falls separate Spalten
            if 'Datum' in processed_df.columns and 'Zeit' in processed_df.columns:
                try:
                    # Kombiniere Datum und Zeit
                    datetime_combined = pd.to_datetime(
                        processed_df['Datum'].astype(str) + ' ' + processed_df['Zeit'].astype(str),
                        errors='coerce'
                    )
                    processed_df['DateTime_Combined'] = datetime_combined
                except Exception as e:
                    logger.warning(f"Datum/Zeit-Kombination fehlgeschlagen: {e}")
            elif 'Datum' in processed_df.columns:
                try:
                    processed_df['Datum'] = pd.to_datetime(
                        processed_df['Datum'], 
                        errors='coerce'
                    )
                except Exception as e:
                    logger.warning(f"Datum-Parsing fehlgeschlagen: {e}")
            
            # Leere EintrÃ¤ge behandeln
            for col in expected_columns:
                if col in processed_df.columns:
                    processed_df[col] = processed_df[col].fillna('').astype(str)
            
            # Leere Ereignis-EintrÃ¤ge entfernen
            if 'Ereignis & Massnahme' in processed_df.columns:
                processed_df = processed_df[processed_df['Ereignis & Massnahme'].str.strip() != '']
            
            logger.info(f"Daten verarbeitet: {len(processed_df)} EintrÃ¤ge")
            return processed_df.reset_index(drop=True)
            
        except Exception as e:
            logger.error(f"Fehler bei der Datenverarbeitung: {e}")
            st.error(f"Fehler bei der Datenverarbeitung: {e}")
            return None
    
    def create_embeddings(self, df: pd.DataFrame) -> bool:
        """Erstellt Embeddings fÃ¼r alle EintrÃ¤ge"""
        try:
            with st.spinner('Erstelle Embeddings...'):
                # Kombiniere relevante Felder fÃ¼r bessere Suche
                texts = []
                for _, row in df.iterrows():
                    # Verwende Ihre Logbuch-Struktur
                    parts = []
                    if 'Datum' in row and pd.notna(row['Datum']):
                        parts.append(f"Datum: {row['Datum']}")
                    if 'Zeit' in row and pd.notna(row['Zeit']):
                        parts.append(f"Zeit: {row['Zeit']}")
                    if 'Lot-Nr.' in row and row['Lot-Nr.']:
                        parts.append(f"Lot: {row['Lot-Nr.']}")
                    if 'Subsystem' in row and row['Subsystem']:
                        parts.append(f"System: {row['Subsystem']}")
                    if 'Ereignis & Massnahme' in row and row['Ereignis & Massnahme']:
                        parts.append(f"Ereignis: {row['Ereignis & Massnahme']}")
                    if 'Visum' in row and row['Visum']:
                        parts.append(f"Visum: {row['Visum']}")
                    
                    combined_text = " ".join(parts)
                    texts.append(combined_text)
                
                # Embeddings erstellen
                self.embeddings = self.model.encode(
                    texts,
                    show_progress_bar=True,
                    convert_to_numpy=True,
                    batch_size=32  # Batch-GrÃ¶ÃŸe fÃ¼r bessere Performance
                )
                
                # FAISS Index erstellen
                dimension = self.embeddings.shape[1]
                self.index = faiss.IndexFlatIP(dimension)  # Inner Product fÃ¼r Cosine Similarity
                
                # Normalisiere Embeddings fÃ¼r Cosine Similarity
                faiss.normalize_L2(self.embeddings)
                self.index.add(self.embeddings)
                
                self.df = df
                logger.info(f"Embeddings erstellt: {len(texts)} Texte, Dimension: {dimension}")
                return True
                
        except Exception as e:
            logger.error(f"Fehler beim Erstellen der Embeddings: {e}")
            st.error(f"Fehler beim Erstellen der Embeddings: {e}")
            return False
    
    def determine_optimal_results(self, scores: List[float], query: str) -> int:
        """Bestimmt automatisch die optimale Anzahl von Ergebnissen basierend auf Relevanz"""
        if not scores or len(scores) == 0:
            return 0
        
        scores = np.array(scores)
        
        # Grundschwellenwerte fÃ¼r Relevanz
        high_relevance_threshold = 0.6  # Reduziert fÃ¼r mehr Ergebnisse
        medium_relevance_threshold = 0.4  # Reduziert fÃ¼r mehr Ergebnisse
        min_relevance_threshold = 0.2   # Neuer minimaler Schwellenwert
        
        # ZÃ¤hle relevante Ergebnisse auf verschiedenen Levels
        high_relevant = int(np.sum(scores >= high_relevance_threshold))
        medium_relevant = int(np.sum(scores >= medium_relevance_threshold))
        min_relevant = int(np.sum(scores >= min_relevance_threshold))
        
        # Adaptiver Ansatz: zeige alle relevanten Ergebnisse
        if high_relevant >= 5:
            # Viele hochrelevante: zeige alle + einige mittlere
            optimal_count = min(high_relevant + min(5, medium_relevant - high_relevant), 25)
        elif medium_relevant >= 3:
            # Einige mittelrelevante: zeige alle relevanten
            optimal_count = min(medium_relevant + min(3, min_relevant - medium_relevant), 20)
        elif min_relevant >= 1:
            # Wenige relevante: zeige alle Ã¼ber Mindestschwelle
            optimal_count = min(min_relevant, 15)
        else:
            # Keine wirklich relevanten: zeige Top 5 fÃ¼r Kontext
            optimal_count = min(5, len(scores))
        
        # Query-spezifische Anpassungen
        query_lower = query.lower()
        
        # Bei spezifischen Suchen (Lot-Nummern, Namen, etc.) mehr Ergebnisse
        if any(keyword in query_lower for keyword in ['lot', 'alle', 'list', 'zeige', 'suche']):
            optimal_count = min(optimal_count * 2, 30)
        
        # Bei Zeitbezug mehr Ergebnisse
        if any(keyword in query_lower for keyword in ['wann', 'zeit', 'datum', 'letzte', 'heute', 'gestern']):
            optimal_count = min(optimal_count + 5, 25)
        
        # Minimum und Maximum einhalten
        optimal_count = max(3, min(optimal_count, 35))  # ErhÃ¶htes Maximum
        
        logger.info(f"Automatische Ergebnisanzahl: {optimal_count} (von {len(scores)} verfÃ¼gbar)")
        logger.info(f"Relevanz-Verteilung: Hoch: {high_relevant}, Mittel: {medium_relevant}, Min: {min_relevant}")
        
        return optimal_count
    def semantic_search(self, query: str, max_results: int = 50) -> Tuple[List[int], List[float], int]:
        """FÃ¼hrt semantische Suche durch und bestimmt optimale Ergebnisanzahl"""
        try:
            # Validierung
            if self.index is None or self.model is None:
                logger.error("Index oder Modell nicht verfÃ¼gbar")
                return [], [], 0
            
            # Query Embedding
            query_embedding = self.model.encode([f"Suche: {query}"], convert_to_numpy=True)
            faiss.normalize_L2(query_embedding)
            
            # Begrenze max_results auf verfÃ¼gbare Daten
            available_count = self.index.ntotal
            actual_max = min(max_results, available_count)
            
            # Suche mit mehr Ergebnissen fÃ¼r Analyse
            scores, indices = self.index.search(query_embedding, actual_max)
            
            # Konvertiere zu Python-Listen fÃ¼r sicherere Verarbeitung
            scores_list = [float(score) for score in scores[0]]
            indices_list = [int(idx) for idx in indices[0]]
            
            # Bestimme optimale Anzahl
            optimal_count = self.determine_optimal_results(scores_list, query)
            
            # Stelle sicher, dass optimal_count nicht grÃ¶ÃŸer als verfÃ¼gbare Ergebnisse ist
            optimal_count = min(optimal_count, len(scores_list))
            
            return indices_list[:optimal_count], scores_list[:optimal_count], optimal_count
            
        except Exception as e:
            logger.error(f"Fehler bei der semantischen Suche: {e}")
            return [], [], 0
    
    def query_ollama(self, prompt: str, context: str, model: str = None, timeout: int = 60) -> str:
        """Sendet Anfrage an lokales Ollama LLM"""
        if model is None:
            model = self.ollama_model
            
        try:
            # PrÃ¼fe verschiedene Ollama Endpunkte
            possible_urls = [
                "http://localhost:11434/api/generate",
                "http://127.0.0.1:11434/api/generate"
            ]
            
            # Erweiterte System-Prompts fÃ¼r bessere Analyse
            system_prompt = """Du bist ein Experte fÃ¼r die Analyse von Logbuch-EintrÃ¤gen. 
            
            Analysiere die gegebenen Daten sorgfÃ¤ltig und beantworte die Frage prÃ¤zise und strukturiert. 
            Verwende nur Informationen aus den bereitgestellten Daten.
            
            WICHTIG: Falls die Anfrage zu unspezifisch ist oder mehr Kontext benÃ¶tigt, stelle RÃ¼ckfragen wie:
            - "Welchen Zeitraum mÃ¶chten Sie betrachten?"
            - "Suchen Sie nach einem bestimmten Subsystem?"
            - "MÃ¶chten Sie alle EintrÃ¤ge oder nur Probleme/Erfolge?"
            
            Antworte auf Deutsch und strukturiere deine Antwort klar mit Ãœberschriften und Bullet Points wo sinnvoll."""
            
            full_prompt = f"{system_prompt}\n\nKontext-Daten:\n{context}\n\nBenutzer-Frage: {prompt}\n\nAntwort:"
            
            payload = {
                "model": model,
                "prompt": full_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.2,  # Etwas hÃ¶her fÃ¼r RÃ¼ckfragen
                    "top_p": 0.9,
                    "num_ctx": 6144  # Mehr Kontext fÃ¼r lÃ¤ngere Antworten
                }
            }
            
            # Versuche verschiedene URLs
            for url in possible_urls:
                try:
                    response = requests.post(url, json=payload, timeout=timeout)
                    
                    if response.status_code == 200:
                        result = response.json()
                        return result.get('response', 'Keine Antwort erhalten')
                    else:
                        continue
                        
                except requests.exceptions.RequestException:
                    continue
            
            # Alle URLs fehlgeschlagen - verwende Fallback-Analyse
            return self.fallback_analysis(prompt, context)
                
        except Exception as e:
            logger.error(f"Fehler bei Ollama-Anfrage: {e}")
            return self.fallback_analysis(prompt, context)
    
    def fallback_analysis(self, prompt: str, context: str) -> str:
        """Fallback-Analyse ohne LLM"""
        lines = context.split('\n')
        entry_count = len([line for line in lines if line.startswith('Eintrag')])
        
        return f"""ðŸ“Š **Automatische Analyse** (Ollama nicht verfÃ¼gbar)

**Gefundene EintrÃ¤ge:** {entry_count}

**Zusammenfassung der Suchergebnisse:**
{context[:500]}{'...' if len(context) > 500 else ''}

ðŸ’¡ **Hinweis:** FÃ¼r detaillierte KI-Analyse starten Sie Ollama mit: `ollama serve`
"""
    
    def analyze_query(self, query: str) -> dict:
        """Hauptfunktion fÃ¼r die intelligente Analyse"""
        if self.df is None or self.index is None:
            return {"error": "Keine Daten geladen"}
        
        try:
            logger.info(f"Starte Analyse fÃ¼r Query: '{query}'")
            
            # Intelligente semantische Suche
            indices, scores, result_count = self.semantic_search(query)
            
            if not indices or result_count == 0:
                return {"error": "Keine relevanten Ergebnisse gefunden"}
            
            logger.info(f"Gefunden: {result_count} relevante EintrÃ¤ge")
            
            # Relevante EintrÃ¤ge extrahieren
            relevant_entries = self.df.iloc[indices].copy()
            relevant_entries['similarity_score'] = scores
            
            # Kontext fÃ¼r LLM vorbereiten
            context_parts = []
            for i, (_, row) in enumerate(relevant_entries.iterrows(), 1):
                context_parts.append(
                    f"Eintrag {i}:\n"
                    f"- Datum: {row.get('Datum', 'N/A')}\n"
                    f"- Zeit: {row.get('Zeit', 'N/A')}\n"
                    f"- Lot-Nr.: {row.get('Lot-Nr.', 'N/A')}\n"
                    f"- Subsystem: {row.get('Subsystem', 'N/A')}\n"
                    f"- Ereignis & Massnahme: {row.get('Ereignis & Massnahme', 'N/A')}\n"
                    f"- Visum: {row.get('Visum', 'N/A')}\n"
                    f"- Relevanz: {scores[i-1]:.3f}\n"
                )
            
            context = "\n".join(context_parts)
            
            result = {
                "relevant_entries": relevant_entries,
                "context": context,
                "query": query,
                "result_count": result_count,
                "total_available": len(self.df)
            }
            
            # Automatische LLM-Analyse
            logger.info("Starte LLM-Analyse...")
            llm_response = self.query_ollama(query, context)
            result["llm_analysis"] = llm_response
            
            return result
            
        except Exception as e:
            logger.error(f"Fehler bei der Analyse: {e}")
            return {"error": f"Fehler bei der Analyse: {str(e)}"}

def main():
    st.set_page_config(
        page_title="Logbuch Analyzer", 
        page_icon="ðŸ“‹", 
        layout="wide"
    )
    
    st.title("ðŸ“‹ Logbuch Analyzer")
    st.markdown("**Intelligente KI-gestÃ¼tzte Suche und Analyse von Logbuch-EintrÃ¤gen**")
    
    # Session State initialisieren
    if 'analyzer' not in st.session_state:
        st.session_state.analyzer = LogbookAnalyzer()
        st.session_state.initialized = False
    
    analyzer = st.session_state.analyzer
    
    # Auto-Initialisierung beim ersten Start
    if not st.session_state.initialized:
        st.info("ðŸš€ Initialisiere Anwendung...")
        success = analyzer.auto_initialize()
        st.session_state.initialized = True
        if success:
            st.rerun()  # Refresh UI nach Initialisierung
    
    # Sidebar fÃ¼r System-Info und Logs
    with st.sidebar:
        st.header("ðŸ”§ System Status")
        
        # Embedding-Modell Status
        if analyzer.model is not None:
            st.success("âœ… Embedding-Modell: Aktiv")
        else:
            st.error("âŒ Embedding-Modell: Nicht geladen")
        
        # Ollama Status
        st.write(f"ðŸ¦™ **Ollama:** {analyzer.ollama_status}")
        
        # Daten Status
        if analyzer.df is not None:
            st.success(f"ðŸ“Š Daten: {len(analyzer.df)} EintrÃ¤ge")
        else:
            st.info("ðŸ“Š Daten: Nicht geladen")
        
        st.markdown("---")
        
        # System Logs
        st.subheader("ðŸ“ System Logs")
        if hasattr(st.session_state, 'log_handler'):
            logs = st.session_state.log_handler.logs[-10:]  # Nur die letzten 10
            if logs:
                log_container = st.container()
                with log_container:
                    for log in logs:
                        st.text(log)
            else:
                st.text("Keine Logs verfÃ¼gbar")
        
        # Refresh-Button
        if st.button("ðŸ”„ System neu prÃ¼fen"):
            analyzer.check_ollama_status()
            st.rerun()
        
        st.markdown("---")
        st.markdown("**ðŸ’¡ Tipps:**")
        st.markdown("- Ollama Status: http://localhost:11434")
        st.markdown("- Automatische Ergebnisoptimierung")
        st.markdown("- KI-Analyse immer aktiv")
    
    # Hauptbereich
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.header("ðŸ“¤ Daten Upload")
        
        uploaded_file = st.file_uploader(
            "CSV-Datei hochladen",
            type=['csv'],
            help="Erwartete Spalten: Datum, Zeit, Lot-Nr., Subsystem, Ereignis & Massnahme, Visum"
        )
        
        if uploaded_file is not None:
            try:
                # CSV mit verschiedenen Trennzeichen versuchen
                separators = [',', ';', ' ', '\t']
                df = None
                successful_separator = None
                
                for sep in separators:
                    try:
                        uploaded_file.seek(0)  # ZurÃ¼ck zum Anfang
                        test_df = pd.read_csv(uploaded_file, sep=sep, encoding='utf-8')
                        
                        # PrÃ¼fe ob DataFrame sinnvoll ist (mehr als 1 Spalte)
                        if len(test_df.columns) > 1:
                            df = test_df
                            successful_separator = sep
                            break
                    except:
                        continue
                
                # Falls UTF-8 fehlschlÃ¤gt, versuche andere Encodings
                if df is None:
                    encodings = ['latin1', 'cp1252', 'iso-8859-1']
                    for encoding in encodings:
                        for sep in separators:
                            try:
                                uploaded_file.seek(0)
                                test_df = pd.read_csv(uploaded_file, sep=sep, encoding=encoding)
                                if len(test_df.columns) > 1:
                                    df = test_df
                                    successful_separator = sep
                                    st.info(f"Datei mit Encoding '{encoding}' und Trennzeichen '{sep}' gelesen")
                                    break
                            except:
                                continue
                        if df is not None:
                            break
                
                if df is not None:
                    st.success(f"âœ… {len(df)} Zeilen geladen (Trennzeichen: '{successful_separator}')")
                
                # Datenvorschau
                with st.expander("ðŸ“Š Datenvorschau"):
                    st.dataframe(df.head())
                    st.write(f"Spalten: {list(df.columns)}")
                
                # Daten verarbeiten
                if st.button("ðŸ”„ Daten verarbeiten"):
                    processed_df = analyzer.preprocess_data(df)
                    if processed_df is not None:
                        st.success(f"âœ… {len(processed_df)} EintrÃ¤ge verarbeitet")
                        
                        # Embeddings erstellen
                        if analyzer.model is not None:
                            if analyzer.create_embeddings(processed_df):
                                st.success("âœ… Embeddings erstellt - Bereit fÃ¼r Suche!")
                        else:
                            st.warning("âš ï¸ Bitte laden Sie zuerst das Embedding-Modell")
                else:
                    st.error("âŒ CSV-Datei konnte nicht gelesen werden. Bitte prÃ¼fen Sie das Format.")
                            
            except Exception as e:
                st.error(f"Fehler beim Laden der CSV: {e}")
                st.info("ðŸ’¡ Tipp: PrÃ¼fen Sie Trennzeichen (Komma, Semikolon) und Encoding (UTF-8, Latin1)")
    
    with col2:
        st.header("ðŸ” Suche & Analyse")
        
        if analyzer.df is not None:
            st.success(f"ðŸ“Š {len(analyzer.df)} EintrÃ¤ge bereit fÃ¼r Suche")
            
            # Suchbereich
            query = st.text_area(
                "Ihre Frage/Suchanfrage:",
                placeholder="z.B. 'Alle EintrÃ¤ge mit Problemen in Lot 12345' oder 'Wann gab es QualitÃ¤tsprobleme?'",
                height=100
            )
            
            if st.button("ðŸ” Suchen", type="primary") and query:
                with st.spinner("Suche lÃ¤uft..."):
                    results = analyzer.analyze_query(query)
                    
                    if "error" in results:
                        st.error(results["error"])
                    else:
                        # LLM Antwort (falls verfÃ¼gbar)
                        if "llm_analysis" in results:
                            st.subheader("ðŸ¤– KI-Analyse")
                            st.write(results["llm_analysis"])
                            st.markdown("---")
                        
                        # Suchergebnisse
                        st.subheader("ðŸ“‹ Gefundene EintrÃ¤ge")
                        
                        relevant_df = results["relevant_entries"]
                        
                        for i, (_, row) in enumerate(relevant_df.iterrows(), 1):
                            # Relevanz-basierte Farbe
                            score = row['similarity_score']
                            if score >= 0.7:
                                score_color = "ðŸŸ¢"  # Hoch relevant
                            elif score >= 0.5:
                                score_color = "ðŸŸ¡"  # Mittel relevant
                            else:
                                score_color = "ðŸŸ "  # Niedrig relevant
                            
                            with st.expander(f"{score_color} Eintrag {i} (Relevanz: {score:.3f})"):
                                col_a, col_b, col_c = st.columns(3)
                                with col_a:
                                    st.write(f"**Datum:** {row.get('Datum', 'N/A')}")
                                    st.write(f"**Zeit:** {row.get('Zeit', 'N/A')}")
                                with col_b:
                                    st.write(f"**Lot-Nr.:** {row.get('Lot-Nr.', 'N/A')}")
                                    st.write(f"**Subsystem:** {row.get('Subsystem', 'N/A')}")
                                with col_c:
                                    st.write(f"**Visum:** {row.get('Visum', 'N/A')}")
                                    st.write(f"**Relevanz:** {score_color} {score:.3f}")
                                
                                st.write(f"**Ereignis & MaÃŸnahme:** {row.get('Ereignis & Massnahme', 'N/A')}")
                        
                        # Download-Option
                        csv = relevant_df.to_csv(index=False)
                        st.download_button(
                            label="ðŸ“¥ Ergebnisse als CSV herunterladen",
                            data=csv,
                            file_name=f"suchergebnisse_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                            mime="text/csv"
                        )
        
        else:
            st.info("ðŸ‘† Bitte laden Sie zuerst eine CSV-Datei hoch")

if __name__ == "__main__":
    main()